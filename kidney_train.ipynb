{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tifffile as tiff\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import *\n",
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet\n",
    "print('tensorflow version:', tf.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 'v5'\n",
    "DATA_PATH = './data'\n",
    "IMGS_PATH = './data/tiles'\n",
    "MSKS_PATH = './data/masks'\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "PARAMS = {\n",
    "    'version': VER,\n",
    "    'folds': 4,\n",
    "    'img_size': 256,\n",
    "    'resize': 4,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 1000,\n",
    "    'patience': 16,\n",
    "    'decay': False,\n",
    "    'backbone': 'efficientnetb5',\n",
    "    'bce_weight': .5,\n",
    "    'seed': 2020\n",
    "}\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "    \n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_all(PARAMS['seed'])\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_masks = pd.read_csv(f'{DATA_PATH}/train.csv').set_index('id')\n",
    "df_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc): continue\n",
    "        s = enc.split()\n",
    "        for i in range(len(s) // 2):\n",
    "            start = int(s[2 * i]) - 1\n",
    "            length = int(s[2 * i + 1])\n",
    "            img[start : start + length] = 1 + m\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def show_img_n_mask(df, img_num, resize):\n",
    "    img = tiff.imread(os.path.join('./data/train', df.index[img_num] + '.tiff'))\n",
    "    if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1, 2, 0))\n",
    "    mask = enc2mask(df.iloc[img_num], (img.shape[1], img.shape[0]))\n",
    "    print(img.shape, mask.shape)\n",
    "    img = cv2.resize(img,\n",
    "                     (img.shape[1] // resize, img.shape[0] // resize),\n",
    "                     interpolation=cv2.INTER_AREA)\n",
    "    mask = cv2.resize(mask,\n",
    "                      (mask.shape[1] // resize, mask.shape[0] // resize),\n",
    "                      interpolation=cv2.INTER_NEAREST)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, alpha=.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_img_n_mask(df=df_masks, img_num=4, resize=PARAMS['resize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = albu.Compose(\n",
    "    [\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightness(limit=.15), \n",
    "                albu.RandomContrast(limit=.15), \n",
    "                albu.RandomGamma()\n",
    "            ], \n",
    "            p=.33\n",
    "        ),\n",
    "        albu.RandomRotate90(p=.33),\n",
    "        albu.HorizontalFlip(p=.33),\n",
    "        albu.VerticalFlip(p=.33),\n",
    "        albu.ShiftScaleRotate(shift_limit=.1, scale_limit=.1, rotate_limit=20, p=.33)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenKid(Sequence):\n",
    "    \n",
    "    def __init__(self, imgs_path, msks_path, imgs_idxs, img_size,\n",
    "                 batch_size=32, mode='fit', shuffle=False, \n",
    "                 aug=None, resize=None):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.msks_path = msks_path\n",
    "        self.imgs_idxs = imgs_idxs\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.aug = aug\n",
    "        self.resize = resize\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.imgs_idxs) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.imgs_idxs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        batch_size = min(self.batch_size, len(self.imgs_idxs) - index*self.batch_size)\n",
    "        X = np.zeros((batch_size, self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        imgs_batch = self.imgs_idxs[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        if self.mode == 'fit':\n",
    "            y = np.zeros((batch_size, self.img_size, self.img_size), dtype=np.float32)\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ], y[i] = self.get_tile(img_idx)\n",
    "            return X, y\n",
    "        elif self.mode == 'predict':\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ] = self.get_tile(img_idx)\n",
    "            return X\n",
    "        else:\n",
    "            raise AttributeError('mode parameter error')\n",
    "            \n",
    "    def get_tile(self, img_idx):\n",
    "        img_path = f'{self.imgs_path}/{img_idx}.png'\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print('error load image:', img_path)\n",
    "        if self.resize:\n",
    "            img = cv2.resize(img, (int(img.shape[1] / self.resize), int(img.shape[0] / self.resize)))\n",
    "        img = img.astype(np.float32) / 255\n",
    "        if self.mode == 'fit':\n",
    "            msk_path = f'{self.msks_path}/{img_idx}.png'\n",
    "            msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if msk is None:\n",
    "                print('error load mask:', msk_path)\n",
    "            if self.resize:\n",
    "                msk = cv2.resize(msk, (int(msk.shape[1] / self.resize), int(msk.shape[0] / self.resize)))\n",
    "            msk = msk.astype(np.float32)\n",
    "            if self.aug:\n",
    "                augmented = self.aug(image=img, mask=msk)\n",
    "                img = augmented['image']\n",
    "                msk = augmented['mask']\n",
    "            return img, msk\n",
    "        else:\n",
    "            if self.aug:\n",
    "                img = self.aug(image=img)['image']\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_idxs = [x.replace('.png', '') for x in os.listdir(IMGS_PATH) if '.png' in x]\n",
    "train_datagen = DataGenKid(\n",
    "        imgs_path=IMGS_PATH, \n",
    "        msks_path=MSKS_PATH, \n",
    "        imgs_idxs=imgs_idxs, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=True,           \n",
    "        aug=aug, \n",
    "        resize=None\n",
    ")\n",
    "val_datagen = DataGenKid(\n",
    "    imgs_path=IMGS_PATH, \n",
    "    msks_path=MSKS_PATH, \n",
    "    imgs_idxs=imgs_idxs, \n",
    "    img_size=PARAMS['img_size'], \n",
    "    batch_size=PARAMS['batch_size'], \n",
    "    mode='fit', \n",
    "    shuffle=False,           \n",
    "    aug=None, \n",
    "    resize=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 8\n",
    "Xt, yt = train_datagen.__getitem__(3)\n",
    "print('test X: ', Xt.shape)\n",
    "print('test y: ', yt.shape)\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=2, ncols=bsize)\n",
    "for j in range(bsize):\n",
    "    axes[0, j].imshow(Xt[j])\n",
    "    axes[0, j].set_title(j)\n",
    "    axes[0, j].axis('off')\n",
    "    axes[1, j].imshow(yt[j])\n",
    "    axes[1, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 8\n",
    "Xt, yt = val_datagen.__getitem__(5)\n",
    "print('test X: ', Xt.shape)\n",
    "print('test y: ', yt.shape)\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=2, ncols=bsize)\n",
    "for j in range(bsize):\n",
    "    axes[0, j].imshow(Xt[j])\n",
    "    axes[0, j].set_title(j)\n",
    "    axes[0, j].axis('off')\n",
    "    axes[1, j].imshow(yt[j])\n",
    "    axes[1, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1):\n",
    "    return (1 - dice_coef(y_true, y_pred, smooth))\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return PARAMS['bce_weight'] * binary_crossentropy(y_true, y_pred) + \\\n",
    "    (1 - PARAMS['bce_weight']) * dice_loss(y_true, y_pred)\n",
    "\n",
    "def get_model(backbone, input_shape, classes=1, learning_rate=.001):\n",
    "    model = Unet(\n",
    "            backbone_name=backbone,\n",
    "            input_shape=input_shape,\n",
    "            classes=classes, \n",
    "            activation='sigmoid'\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(\n",
    "            tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            sync_period=10\n",
    "        ),\n",
    "        loss=bce_dice_loss, \n",
    "        metrics=[dice_coef]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=10, epochs=100, warmup=5, plot=False):\n",
    "    lr_start = 1e-5\n",
    "    lr_max = 1e-3\n",
    "    lr_min = lr_start / 100\n",
    "    lr_ramp_ep = warmup\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = .95\n",
    "    \n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "        \n",
    "    if not plot:\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=False)\n",
    "        return lr_callback \n",
    "    else: \n",
    "        return lr_scheduler\n",
    "    \n",
    "if PARAMS['decay']:\n",
    "    lr_scheduler_plot = get_lr_callback(\n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        epochs=PARAMS['epochs'], \n",
    "        plot=True\n",
    "    )\n",
    "    xs = [i for i in range(PARAMS['epochs'])]\n",
    "    y = [lr_scheduler_plot(x) for x in xs]\n",
    "    plt.plot(xs, y)\n",
    "    plt.title(f'lr schedule from {y[0]:.5f} to {max(y):.3f} to {y[-1]:.8f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mparams, n_fold, train_datagen, val_datagen):\n",
    "    model = get_model(\n",
    "        mparams['backbone'], \n",
    "        input_shape=(mparams['img_size'], mparams['img_size'], 3)\n",
    "    )\n",
    "    checkpoint_path = f'{MDLS_PATH}/model_{n_fold}.hdf5'\n",
    "    earlystopper = EarlyStopping(\n",
    "        monitor='val_dice_coef', \n",
    "        patience=mparams['patience'], \n",
    "        verbose=0,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    lrreducer = ReduceLROnPlateau(\n",
    "        monitor='val_dice_coef', \n",
    "        factor=.1, \n",
    "        patience=int(mparams['patience'] / 2), \n",
    "        verbose=0, \n",
    "        min_lr=1e-7,\n",
    "        mode='max'\n",
    "    )\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        monitor='val_dice_coef', \n",
    "        verbose=0, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True, \n",
    "        mode='max'\n",
    "    )\n",
    "    callbacks = [earlystopper, checkpointer]\n",
    "    if mparams['decay']:\n",
    "        callbacks.append(get_lr_callback(mparams['batch_size']))\n",
    "        print('lr warmup and decay')\n",
    "    else:\n",
    "        callbacks.append(lrreducer)\n",
    "        print('lr reduce on plateau')\n",
    "    history = model.fit(\n",
    "        train_datagen,\n",
    "        validation_data=val_datagen,\n",
    "        callbacks=callbacks,\n",
    "        epochs=mparams['epochs'],\n",
    "        verbose=1\n",
    "    )\n",
    "    history_file = f'{MDLS_PATH}/history_{n_fold}.json'\n",
    "    dict_to_save = {}\n",
    "    for k, v in history.history.items():\n",
    "        dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\n",
    "    with open(history_file, 'w') as file:\n",
    "        json.dump(dict_to_save, file)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=PARAMS['folds'],     \n",
    "              random_state=PARAMS['seed'],\n",
    "              shuffle=True).split(imgs_idxs)\n",
    "bavg_epoch = 0\n",
    "bavg_loss = 0\n",
    "bavg_dice_coef = 0\n",
    "\n",
    "for n, (tr, te) in enumerate(kfold):\n",
    "    print('=' * 10, f'FOLD {n}', '=' * 10)\n",
    "    X_tr = [imgs_idxs[i] for i in tr]; X_val = [imgs_idxs[i] for i in te]\n",
    "    print('train:', len(X_tr), '| test:', len(X_val))\n",
    "    train_datagen = DataGenKid(\n",
    "        imgs_path=IMGS_PATH, \n",
    "        msks_path=MSKS_PATH, \n",
    "        imgs_idxs=X_tr, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=True,           \n",
    "        aug=aug, \n",
    "        resize=None\n",
    "    )\n",
    "    val_datagen = DataGenKid(\n",
    "        imgs_path=IMGS_PATH, \n",
    "        msks_path=MSKS_PATH, \n",
    "        imgs_idxs=X_val, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=False,           \n",
    "        aug=None, \n",
    "        resize=None\n",
    "    )\n",
    "    model, history = train_model(PARAMS, n, train_datagen, val_datagen)\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(history.history['dice_coef'], label='dice_coef')\n",
    "    plt.plot(history.history['val_dice_coef'], label='val_dice_coef')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    best_loss = history.history['val_loss'][best_epoch]\n",
    "    best_dice_coef = history.history['val_dice_coef'][best_epoch]\n",
    "    print('best epoch:', best_epoch, \n",
    "          '| best loss:', best_loss,\n",
    "          '| best dice coef:', best_dice_coef)\n",
    "    bavg_epoch = bavg_epoch + (best_epoch / PARAMS['folds'])\n",
    "    bavg_loss = bavg_loss + (best_loss / PARAMS['folds'])\n",
    "    bavg_dice_coef = bavg_dice_coef + (best_dice_coef / PARAMS['folds'])\n",
    "    \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = PARAMS.copy()\n",
    "result['bavg_epoch'] = bavg_epoch\n",
    "result['bavg_loss'] = bavg_loss\n",
    "result['bavg_dice_coef'] = bavg_dice_coef\n",
    "if not os.path.exists('results.csv'):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv('results.csv', sep='\\t')\n",
    "else:\n",
    "    df_old = pd.read_csv('results.csv', sep='\\t', index_col=0)\n",
    "    df_save = pd.DataFrame(result, index=[df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv('results.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('results.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 0\n",
    "larger = 1\n",
    "checkpoint_path = f'{MDLS_PATH}/model_{n_fold}.hdf5'\n",
    "model_lrg = get_model(\n",
    "    PARAMS['backbone'], \n",
    "    input_shape=(PARAMS['img_size'] * larger, PARAMS['img_size'] * larger, 3)\n",
    ")\n",
    "model_lrg.load_weights(checkpoint_path) # or .set_weights(model.get_weights()) from smaller model\n",
    "model_lrg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 16\n",
    "val_datagen = DataGenKid(\n",
    "    imgs_path=IMGS_PATH, \n",
    "    msks_path=MSKS_PATH, \n",
    "    imgs_idxs=imgs_idxs, \n",
    "    img_size=PARAMS['img_size'], \n",
    "    batch_size=bsize, \n",
    "    mode='fit', \n",
    "    shuffle=False,           \n",
    "    aug=None, \n",
    "    resize=None\n",
    ")\n",
    "Xt, yt = val_datagen.__getitem__(0)\n",
    "y_pred = model_lrg.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('test X: ', Xt.shape)\n",
    "print('test y: ', yt.shape)\n",
    "print('pred y: ', y_pred.shape)\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=3, ncols=bsize)\n",
    "for j in range(bsize):\n",
    "    axes[0, j].imshow(Xt[j])\n",
    "    axes[0, j].set_title(j)\n",
    "    axes[0, j].axis('off')\n",
    "    axes[1, j].set_title('true')\n",
    "    axes[1, j].imshow(yt[j])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[2, j].set_title('pred')\n",
    "    axes[2, j].imshow(np.squeeze(y_pred[j]))\n",
    "    axes[2, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 0\n",
    "larger = 4\n",
    "checkpoint_path = f'{MDLS_PATH}/model_{n_fold}.hdf5'\n",
    "model_lrg = get_model(\n",
    "    PARAMS['backbone'], \n",
    "    input_shape=(PARAMS['img_size'] * larger, PARAMS['img_size'] * larger, 3)\n",
    ")\n",
    "model_lrg.load_weights(checkpoint_path) # or .set_weights(model.get_weights()) from smaller model\n",
    "model_lrg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "resize = 4\n",
    "shft = .6\n",
    "wnd = PARAMS['img_size'] * larger\n",
    "img = tiff.imread(os.path.join('./data/train', df_masks.index[img_num] + '.tiff'))\n",
    "if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1, 2, 0))\n",
    "mask = enc2mask(df_masks.iloc[img_num], (img.shape[1], img.shape[0]))\n",
    "print(img.shape, mask.shape)\n",
    "img = cv2.resize(img,\n",
    "                 (img.shape[1] // resize, img.shape[0] // resize),\n",
    "                 interpolation=cv2.INTER_AREA)\n",
    "mask = cv2.resize(mask,\n",
    "                  (mask.shape[1] // resize, mask.shape[0] // resize),\n",
    "                  interpolation=cv2.INTER_NEAREST)\n",
    "img = img[int(img.shape[0]*shft) : int(img.shape[0]*shft)+wnd, \n",
    "          int(img.shape[1]*shft) : int(img.shape[1]*shft)+wnd, \n",
    "          :]\n",
    "mask = mask[int(mask.shape[0]*shft) : int(mask.shape[0]*shft)+wnd, \n",
    "            int(mask.shape[1]*shft) : int(mask.shape[1]*shft)+wnd]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.imshow(mask, alpha=.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lrg = model_lrg.predict(img[np.newaxis, ] / 255)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.imshow(np.squeeze(mask_lrg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.hist(mask_lrg.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
